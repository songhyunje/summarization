import argparse
import glob
import logging
import os

import numpy as np
import torch

from src.common.score import rouge_scorer
from src.triple_summary.triplesum import TripleSum
from src.triple_summary.translator import build_predictor

DEFAULT_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

parser = argparse.ArgumentParser()
parser.add_argument(
    "--checkpoint_dir",
    default=None,
    type=str,
    required=True,
    help="The output directory where the rnn predictions and checkpoints will be written.")
parser.add_argument(
    "--alpha",
    default=0.95,
    type=float,
    help="The value of alpha for the length penalty in the beam search.",
)
parser.add_argument("--input", default='../../input.txt', type=str)
parser.add_argument("--result", default='result.txt', type=str)
parser.add_argument(
    "--beam_size", default=5, type=int, help="The number of beams to start with for each example.",
)
parser.add_argument(
    "--min_length", default=30, type=int, help="Minimum number of tokens for the summaries.",
)
parser.add_argument(
    "--max_length", default=100, type=int, help="Maixmum number of tokens for the summaries.",
)
parser.add_argument(
    "--block_trigram",
    action="store_true",
    help="Whether to block the existence of repeating trigrams in the text generated by beam search.",
)
parser.add_argument(
    "--device", type=str, required=False, default=DEFAULT_DEVICE, help="cuda, cuda:1, cpu etc.",
)

args = parser.parse_args()

logging.basicConfig(level=getattr(logging, 'INFO'))
logger = logging.getLogger(__name__)

checkpoints = list(sorted(glob.glob(os.path.join(args.checkpoint_dir, "checkpointepoch=*.ckpt"), recursive=True)))
logger.info("Load model from %s", checkpoints[-1])
logger.info("Device %s", args.device)
summarizer = TripleSum.load_from_checkpoint(checkpoints[-1], map_location=args.device)

tokenizer = summarizer.tokenizer
model = summarizer.model
model.to(args.device)
model.eval()

symbols = {
    "BOS": tokenizer.vocab["<!#s>"],
    "EOS": tokenizer.vocab["<!#/s>"],
    "PAD": tokenizer.vocab["<!#pad>"],
    "PERIOD": tokenizer.vocab["."],
}

translator = build_predictor(args, tokenizer, symbols, model)

rouge1, rougeL, results = [], [], []
with open(args.input) as f:
    for i, line in enumerate(f):
        src, target = line.split('|||||')
        src = src.strip()
        target = target.strip()

        sent_tokenized = tokenizer.encode_plus(src, max_length=512, pad_to_max_length=True,
                                               return_tensors="pt", device=args.device)
        for translate in translator.translate(sent_tokenized):
            pred = tokenizer.decode(translate)

        logger.info("SOURCE: %s", src)
        logger.info("TARGET: %s", target)
        logger.info("PRED: %s", ''.join(pred))
