import argparse
import glob
import logging
import os

import numpy as np
import torch

from src.common.score import rouge_scorer
from src.multi_summary.multisum import MDS
from src.multi_summary.translator import build_predictor

DEFAULT_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

parser = argparse.ArgumentParser()
parser.add_argument(
    "--checkpoint_dir",
    default=None,
    type=str,
    required=True,
    help="The output directory where the model predictions and checkpoints will be written.")
parser.add_argument(
    "--alpha",
    default=0.95,
    type=float,
    help="The value of alpha for the length penalty in the beam search.",
)
parser.add_argument("--input", default='../../data/multi/multi_input.txt', type=str)
parser.add_argument("--result", default='../../data/multi/result_kt_multisum.txt', type=str)
parser.add_argument(
    "--beam_size", default=5, type=int, help="The number of beams to start with for each example.",
)
parser.add_argument(
    "--max_src_seq_length",
    default=512,
    type=int,
    help="The maximum total input sequence length after tokenization. Sequences longer "
         "than this will be truncated, sequences shorter will be padded.",
)
parser.add_argument(
    "--min_length", default=30, type=int, help="Minimum number of tokens for the summaries.",
)
parser.add_argument(
    "--max_length", default=100, type=int, help="Maixmum number of tokens for the summaries.",
)
parser.add_argument(
    "--block_trigram",
    action="store_true",
    help="Whether to block the existence of repeating trigrams in the text generated by beam search.",
)
parser.add_argument(
    "--device", type=str, required=False, default=DEFAULT_DEVICE, help="cuda, cuda:1, cpu etc.",
)

args = parser.parse_args()

logging.basicConfig(level=getattr(logging, 'INFO'))
logger = logging.getLogger(__name__)

checkpoints = list(sorted(glob.glob(os.path.join(args.checkpoint_dir, "checkpointepoch=*.ckpt"), recursive=True)))
logger.info("Load model from %s", checkpoints[-1])
logger.info("Device %s", args.device)
summarizer = MDS.load_from_checkpoint(checkpoints[-1], map_location=args.device)

tokenizer = summarizer.tokenizer
model = summarizer.model
model.to(args.device)
model.eval()

symbols = {
    "BOS": tokenizer.vocab["<!#s>"],
    "EOS": tokenizer.vocab["<!#/s>"],
    "PAD": tokenizer.vocab["<!#pad>"],
    "PERIOD": tokenizer.vocab["."],
}

translator = build_predictor(args, tokenizer, symbols, model)
scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'])

rouge1, rougeL, results = [], [], []
with open(args.input) as f:
    for i, line in enumerate(f):
        tokens = line.split("|||||")
        srcs = tokens[:-1]
        target = tokens[-1].strip()

        sources = []
        for src in srcs:
            sent_tokenized = tokenizer.encode_plus(
                src, max_length=args.max_src_seq_length, pad_to_max_length=True,
                return_tensors="pt", device=args.device
            )
            sources.append(sent_tokenized)

        for translate in translator.translate(sources):
            pred = tokenizer.decode(translate)

        for idx, src in enumerate(srcs, 1):
            logger.info("SOURCE%d: %s", idx, src)
        logger.info("TARGET: %s", ''.join(target))
        logger.info("PRED: %s", ''.join(pred))

        # words_scores = scorer.score(''.join(target), ''.join(pred))
        # rouge1.append(words_scores['rouge1'].fmeasure)
        # rougeL.append(words_scores['rougeL'].fmeasure)
        # results.append(('|||||'.join(srcs), pred))

logger.info("Rouge1: %f, RougeL: %f" % (np.mean(rouge1), np.mean(rougeL)))
logger.info("Write the result to %s", args.result)
with open(args.result, 'w') as f:
    for src, pred in results:
        f.write('%s|||||%s\n' % (src, pred))
